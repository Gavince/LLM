{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbcf08cf-1764-478d-b9f5-a46e1d4edd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd code_analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6357f75-13c4-4fda-867f-fdd3716a009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92458fc5-addb-42b0-9da5-8083485f432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Logger debug_logger (INFO)> <Logger qa_logger (INFO)>\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility, \\\n",
    "    Partition\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import asyncio\n",
    "from functools import partial\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from qanything_kernel.configs.model_config import MILVUS_HOST_LOCAL, MILVUS_HOST_ONLINE, MILVUS_PORT, MILVUS_USER, MILVUS_PASSWORD, MILVUS_DB_NAME, CHUNK_SIZE, VECTOR_SEARCH_TOP_K\n",
    "from qanything_kernel.utils.custom_log import debug_logger\n",
    "from langchain.docstore.document import Document\n",
    "import math\n",
    "from itertools import groupby\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cc3bada-97ae-4f60-95e1-735f8b1a139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusFailed(Exception):\n",
    "    \"\"\"异常基类\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b62d371-8433-49d3-9d32-424af6ec4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusClient:\n",
    "    def __init__(self, mode, user_id, kb_ids, *, threshold=1.1, client_timeout=3):\n",
    "        self.user_id = user_id\n",
    "        self.kb_ids = kb_ids\n",
    "        if mode == 'local':\n",
    "            self.host = MILVUS_HOST_LOCAL\n",
    "        else:\n",
    "            self.host = MILVUS_HOST_ONLINE\n",
    "        self.port = MILVUS_PORT\n",
    "        self.user = MILVUS_USER\n",
    "        self.password = MILVUS_PASSWORD\n",
    "        self.db_name = MILVUS_DB_NAME\n",
    "        self.client_timeout = client_timeout\n",
    "        self.threshold = threshold\n",
    "        self.sess: Collection = None\n",
    "        self.partitions: List[Partition] = []\n",
    "        self.executor = ThreadPoolExecutor(max_workers=10)\n",
    "        self.top_k = VECTOR_SEARCH_TOP_K\n",
    "        self.search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 256}}\n",
    "        if mode == 'local':\n",
    "            self.create_params = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 2048}}\n",
    "        else:\n",
    "            self.create_params = {\"metric_type\": \"L2\", \"index_type\": \"GPU_IVF_FLAT\", \"params\": {\"nlist\": 2048}}\n",
    "        self.last_init_ts = time.time() - 100  # 减去100保证最初的init不会被拒绝\n",
    "        self.init()\n",
    "\n",
    "    @property\n",
    "    def fields(self):\n",
    "        fields = [\n",
    "            FieldSchema(name='chunk_id', dtype=DataType.VARCHAR, max_length=64, is_primary=True),\n",
    "            FieldSchema(name='file_id', dtype=DataType.VARCHAR, max_length=64),\n",
    "            FieldSchema(name='file_name', dtype=DataType.VARCHAR, max_length=640),\n",
    "            FieldSchema(name='file_path', dtype=DataType.VARCHAR, max_length=640),\n",
    "            FieldSchema(name='timestamp', dtype=DataType.VARCHAR, max_length=64),\n",
    "            FieldSchema(name='content', dtype=DataType.VARCHAR, max_length=4000),\n",
    "            FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=768)\n",
    "        ]\n",
    "        return fields\n",
    "\n",
    "    def parse_batch_result(self, batch_result):\n",
    "        new_result = []\n",
    "        for batch_idx, result in enumerate(batch_result):\n",
    "            new_cands = []\n",
    "            result.sort(key=lambda x: x.score)\n",
    "            # 获取指定阈值的数据,若无则直接选取top-k个元素\n",
    "            valid_results = [cand for cand in result if cand.score <= self.threshold]\n",
    "            if len(valid_results) == 0:  # 如果没有合适的结果，就取topk\n",
    "                valid_results = result[:self.top_k]\n",
    "            for cand_i, cand in enumerate(valid_results):\n",
    "                doc = Document(page_content=cand.entity.get('content'),\n",
    "                               metadata={\"score\": cand.score, \"file_id\": cand.entity.get('file_id'),\n",
    "                                         \"file_name\": cand.entity.get('file_name'),\n",
    "                                         \"chunk_id\": cand.entity.get('chunk_id')})\n",
    "                new_cands.append(doc)\n",
    "            # csv和xlsx文件不做expand_cand_docs\n",
    "            need_expand, not_need_expand = [], []\n",
    "            for doc in new_cands:\n",
    "                if doc.metadata['file_name'].lower().split('.')[-1] in ['csv', 'xlsx']:\n",
    "                    doc.metadata[\"kernel\"] = doc.page_content\n",
    "                    not_need_expand.append(doc)\n",
    "                else:\n",
    "                    need_expand.append(doc)\n",
    "            expand_res = self.expand_cand_docs(need_expand)\n",
    "            new_cands = not_need_expand + expand_res\n",
    "            new_result.append(new_cands)\n",
    "        return new_result\n",
    "\n",
    "    @property\n",
    "    def output_fields(self):\n",
    "        return ['chunk_id', 'file_id', 'file_name', 'file_path', 'timestamp', 'content']\n",
    "\n",
    "    def init(self):\n",
    "        try:\n",
    "            connections.connect(host=self.host, port=self.port, user=self.user,\n",
    "                                password=self.password, db_name=self.db_name)  # timeout=3 [cannot set]\n",
    "            if utility.has_collection(self.user_id):\n",
    "                self.sess = Collection(self.user_id)\n",
    "                debug_logger.info(f'collection {self.user_id} exists')\n",
    "            else:\n",
    "                schema = CollectionSchema(self.fields)\n",
    "                debug_logger.info(f'create collection {self.user_id} {schema}')\n",
    "                self.sess = Collection(self.user_id, schema)\n",
    "                self.sess.create_index(field_name=\"embedding\", index_params=self.create_params)\n",
    "            for kb_id in self.kb_ids:\n",
    "                if not self.sess.has_partition(kb_id):\n",
    "                    self.sess.create_partition(kb_id)\n",
    "            self.partitions = [Partition(self.sess, kb_id) for kb_id in self.kb_ids]\n",
    "            debug_logger.info('partitions: %s', self.kb_ids)\n",
    "            self.sess.load()\n",
    "        except Exception as e:\n",
    "            debug_logger.error(e)\n",
    "\n",
    "    def __search_emb_sync(self, embs, expr='', top_k=None, client_timeout=None):\n",
    "        if not top_k:\n",
    "            top_k = self.top_k\n",
    "        milvus_records = self.sess.search(data=embs, partition_names=self.kb_ids, anns_field=\"embedding\",\n",
    "                                          param=self.search_params, limit=top_k,\n",
    "                                          output_fields=self.output_fields, expr=expr, timeout=client_timeout)\n",
    "        # debug_logger.info(milvus_records)\n",
    "        return self.parse_batch_result(milvus_records)\n",
    "\n",
    "    def search_emb_async(self, embs, expr='', top_k=None, client_timeout=None):\n",
    "        if not top_k:\n",
    "            top_k = self.top_k\n",
    "        # 将search_emb_sync函数放入线程池中运行\n",
    "        future = self.executor.submit(self.__search_emb_sync, embs, expr, top_k, client_timeout)\n",
    "        return future.result()\n",
    "\n",
    "    def query_expr_async(self, expr, output_fields=None, client_timeout=None):\n",
    "        if client_timeout is None:\n",
    "            client_timeout = self.client_timeout\n",
    "        if not output_fields:\n",
    "            output_fields = self.output_fields\n",
    "        future = self.executor.submit(\n",
    "            partial(self.sess.query, partition_names=self.kb_ids, output_fields=output_fields, expr=expr,\n",
    "                    timeout=client_timeout))\n",
    "        return future.result()\n",
    "\n",
    "    async def insert_files(self, file_id, file_name, file_path, docs, embs, batch_size=1000):\n",
    "        debug_logger.info(f'now inser_file {file_name}')\n",
    "        now = datetime.now()\n",
    "        timestamp = now.strftime(\"%Y%m%d%H%M\")\n",
    "        loop = asyncio.get_running_loop()\n",
    "        contents = [doc.page_content for doc in docs]\n",
    "        num_docs = len(docs)\n",
    "        for batch_start in range(0, num_docs, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, num_docs)\n",
    "            data = [[] for _ in range(len(self.sess.schema))]\n",
    "\n",
    "            for idx in range(batch_start, batch_end):\n",
    "                cont = contents[idx]\n",
    "                emb = embs[idx]\n",
    "                chunk_id = f'{file_id}_{idx}'\n",
    "                data[0].append(chunk_id)\n",
    "                data[1].append(file_id)\n",
    "                data[2].append(file_name)\n",
    "                data[3].append(file_path)\n",
    "                data[4].append(timestamp)\n",
    "                data[5].append(cont)\n",
    "                data[6].append(emb)\n",
    "\n",
    "            # 执行插入操作\n",
    "            try:\n",
    "                debug_logger.info('Inserting into Milvus...')\n",
    "                mr = await loop.run_in_executor(\n",
    "                    self.executor, partial(self.partitions[0].insert, data=data))\n",
    "                debug_logger.info(f'{file_name} {mr}')\n",
    "            except Exception as e:\n",
    "                debug_logger.error(f'Milvus insert file_id:{file_id}, file_name:{file_name} failed: {e}')\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def delete_collection(self):\n",
    "        self.sess.release()\n",
    "        utility.drop_collection(self.user_id)\n",
    "\n",
    "    def delete_partition(self, partition_name):\n",
    "        part = Partition(self.sess, partition_name)\n",
    "        part.release()\n",
    "        self.sess.drop_partition(partition_name)\n",
    "\n",
    "    def delete_files(self, files_id):\n",
    "        self.sess.delete(expr=f\"file_id in {files_id}\")\n",
    "        debug_logger.info('milvus delete files_id: %s', files_id)\n",
    "\n",
    "    def get_files(self, files_id):\n",
    "        res = self.query_expr_async(expr=f\"file_id in {files_id}\", output_fields=[\"file_id\"])\n",
    "        valid_ids = [result['file_id'] for result in res]\n",
    "        return valid_ids\n",
    "\n",
    "    def seperate_list(self, ls: List[int]) -> List[List[int]]:\n",
    "        lists = []\n",
    "        ls1 = [ls[0]]\n",
    "        for i in range(1, len(ls)):\n",
    "            if ls[i - 1] + 1 == ls[i]:\n",
    "                ls1.append(ls[i])\n",
    "            else:\n",
    "                lists.append(ls1)\n",
    "                ls1 = [ls[i]]\n",
    "        lists.append(ls1)\n",
    "        return lists\n",
    "\n",
    "    def process_group(self, group):\n",
    "        new_cands = []\n",
    "        # 对每个分组按照chunk_id进行排序\n",
    "        group.sort(key=lambda x: int(x.metadata['chunk_id'].split('_')[-1]))\n",
    "        id_set = set()\n",
    "        file_id = group[0].metadata['file_id']\n",
    "        file_name = group[0].metadata['file_name']\n",
    "        group_scores_map = {}\n",
    "        # 先找出该文件所有需要搜索的chunk_id\n",
    "        cand_chunks_set = set()  # 使用集合而不是列表\n",
    "        for cand_doc in group:\n",
    "            current_chunk_id = int(cand_doc.metadata['chunk_id'].split('_')[-1])\n",
    "            group_scores_map[current_chunk_id] = cand_doc.metadata['score']\n",
    "            # 使用 set comprehension 一次性生成区间内所有可能的 chunk_id\n",
    "            chunk_ids = {file_id + '_' + str(i) for i in range(current_chunk_id - 200, current_chunk_id + 200)}\n",
    "            # 更新 cand_chunks_set 集合\n",
    "            cand_chunks_set.update(chunk_ids)\n",
    "\n",
    "        cand_chunks = list(cand_chunks_set)\n",
    "\n",
    "        group_relative_chunks = self.query_expr_async(expr=f\"file_id == \\\"{file_id}\\\" and chunk_id in {cand_chunks}\",\n",
    "                                                      output_fields=[\"chunk_id\", \"content\"])\n",
    "        group_chunk_map = {int(item['chunk_id'].split('_')[-1]): item['content'] for item in group_relative_chunks}\n",
    "        group_file_chunk_num = list(group_chunk_map.keys())\n",
    "        for cand_doc in group:\n",
    "            current_chunk_id = int(cand_doc.metadata['chunk_id'].split('_')[-1])\n",
    "            doc = copy.deepcopy(cand_doc)\n",
    "            id_set.add(current_chunk_id)\n",
    "            docs_len = len(doc.page_content)\n",
    "            for k in range(1, 200):\n",
    "                break_flag = False\n",
    "                for expand_index in [current_chunk_id + k, current_chunk_id - k]:\n",
    "                    if expand_index in group_file_chunk_num:\n",
    "                        merge_content = group_chunk_map[expand_index]\n",
    "                        if docs_len + len(merge_content) > CHUNK_SIZE:\n",
    "                            break_flag = True\n",
    "                            break\n",
    "                        else:\n",
    "                            docs_len += len(merge_content)\n",
    "                            id_set.add(expand_index)\n",
    "                if break_flag:\n",
    "                    break\n",
    "\n",
    "        id_list = sorted(list(id_set))\n",
    "        id_lists = self.seperate_list(id_list)\n",
    "        for id_seq in id_lists:\n",
    "            for id in id_seq:\n",
    "                if id == id_seq[0]:\n",
    "                    doc = Document(page_content=group_chunk_map[id],\n",
    "                                   metadata={\"score\": 0, \"file_id\": file_id,\n",
    "                                             \"file_name\": file_name})\n",
    "                else:\n",
    "                    doc.page_content += \" \" + group_chunk_map[id]\n",
    "            doc_score = min([group_scores_map[id] for id in id_seq if id in group_scores_map])\n",
    "            doc.metadata[\"score\"] = float(format(1 - doc_score / math.sqrt(2), '.4f'))\n",
    "            doc.metadata[\"kernel\"] = '|'.join([group_chunk_map[id] for id in id_seq if id in group_scores_map])\n",
    "            new_cands.append(doc)\n",
    "        return new_cands\n",
    "\n",
    "    def expand_cand_docs(self, cand_docs):\n",
    "        cand_docs = sorted(cand_docs, key=lambda x: x.metadata['file_id'])\n",
    "        # 按照file_id进行分组\n",
    "        m_grouped = [list(group) for key, group in groupby(cand_docs, key=lambda x: x.metadata['file_id'])]\n",
    "        debug_logger.info('milvus group number: %s', len(m_grouped))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = []\n",
    "            for group in m_grouped:\n",
    "                if not group:\n",
    "                    continue\n",
    "                future = executor.submit(self.process_group, group)\n",
    "                futures.append(future)\n",
    "\n",
    "            new_cands = []\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    new_cands.extend(result)\n",
    "            return new_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27ef3d48-d518-4bc5-9d77-5ffdf8842e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_kb = MilvusClient(\"local\", \"zhangwanyu\", [\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7122ef52-be1d-4efd-8b28-561bb75ac204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MilvusClient at 0x7fea423478b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milvus_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f192a25-e364-476c-a839-b2274ad17904",
   "metadata": {},
   "source": [
    "## 有道向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b94967b2-6b62-464b-b99d-c942f4149165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Optional\n",
    "import onnxruntime as ort\n",
    "from tritonclient import utils as client_utils\n",
    "from tritonclient.grpc import InferenceServerClient, InferInput, InferRequestedOutput\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List\n",
    "from qanything_kernel.connector.embedding.embedding_client import EmbeddingClient\n",
    "from qanything_kernel.configs.model_config import LOCAL_EMBED_SERVICE_URL, LOCAL_EMBED_MODEL_NAME, LOCAL_EMBED_MAX_LENGTH, LOCAL_EMBED_BATCH\n",
    "from qanything_kernel.utils.custom_log import debug_logger\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5385b45-dc07-4779-9f3b-b220deffe364",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT2NPDTYPE = {\n",
    "    \"fp32\": np.float32,\n",
    "    \"fp16\": np.float16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ca4b9ec-60d3-4f9c-bb61-d007747b147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClient:\n",
    "    DEFAULT_MAX_RESP_WAIT_S = 120\n",
    "    embed_version = \"local_v0.0.1_20230525_6d4019f1559aef84abc2ab8257e1ad4c\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        server_url: str,\n",
    "        model_name: str,\n",
    "        model_version: str,\n",
    "        tokenizer_path: str,\n",
    "        resp_wait_s: Optional[float] = None,\n",
    "    ):\n",
    "        self._server_url = server_url\n",
    "        self._model_name = model_name\n",
    "        self._model_version = model_version\n",
    "        self._response_wait_t = self.DEFAULT_MAX_RESP_WAIT_S if resp_wait_s is None else resp_wait_s\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    def get_embedding(self, sentences, max_length=512):\n",
    "        # Setting up client\n",
    "    \n",
    "        inputs_data = self._tokenizer(sentences, padding=True, truncation=True, max_length=max_length, return_tensors='np')\n",
    "        inputs_data = {k: v for k, v in inputs_data.items()}\n",
    "    \n",
    "        client = InferenceServerClient(url=self._server_url)\n",
    "        model_config = client.get_model_config(self._model_name, self._model_version)\n",
    "        model_metadata = client.get_model_metadata(self._model_name, self._model_version)\n",
    "    \n",
    "        inputs_info = {tm.name: tm for tm in model_metadata.inputs}\n",
    "        outputs_info = {tm.name: tm for tm in model_metadata.outputs}\n",
    "        output_names = list(outputs_info)\n",
    "        outputs_req = [InferRequestedOutput(name_) for name_ in outputs_info]\n",
    "        infer_inputs = []\n",
    "        for name_ in inputs_info:\n",
    "            data = inputs_data[name_]\n",
    "            infer_input = InferInput(name_, data.shape, inputs_info[name_].datatype)\n",
    "    \n",
    "            target_np_dtype = client_utils.triton_to_np_dtype(inputs_info[name_].datatype)\n",
    "            data = data.astype(target_np_dtype)\n",
    "    \n",
    "            infer_input.set_data_from_numpy(data)\n",
    "            infer_inputs.append(infer_input)\n",
    "    \n",
    "        results = client.infer(\n",
    "            model_name=self._model_name,\n",
    "            model_version=self._model_version,\n",
    "            inputs=infer_inputs,\n",
    "            outputs=outputs_req,\n",
    "            client_timeout=120,\n",
    "        )\n",
    "        y_pred = {name_: results.as_numpy(name_) for name_ in output_names}\n",
    "        embeddings = y_pred[\"output\"][:,0]\n",
    "        norm_arr = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings_normalized = embeddings / norm_arr\n",
    "        return embeddings_normalized.tolist()\n",
    "    \n",
    "    def getModelVersion(self):\n",
    "        return self.embed_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc449cf6-9f8c-4db4-8390-6cdad0d9ad5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'localhost:None'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOCAL_EMBED_SERVICE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a8d9102-b0af-42d5-8ce4-7637e0b0774e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embed'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOCAL_EMBED_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "854cfbcb-b348-4f75-a463-c450d3487d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_client = EmbeddingClient(\n",
    "    server_url=\"localhost:8000\",\n",
    "    model_name=LOCAL_EMBED_MODEL_NAME,\n",
    "    model_version='1',\n",
    "    resp_wait_s=120,\n",
    "    tokenizer_path='/Users/zhangmoyu/Machine/data/Qanything/bce-embedding-base_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4120caf-09bb-4442-8641-d8e19ae1bc6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "InferenceServerException",
     "evalue": "[StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:8000: Failed to connect to remote host: Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/39x7dcks3qlg8mtfpkhrn96h0000gn/T/ipykernel_87701/4284550580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zhangw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/sh/39x7dcks3qlg8mtfpkhrn96h0000gn/T/ipykernel_87701/2810186606.py\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(self, sentences, max_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInferenceServerClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mmodel_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tritonclient/grpc/_client.py\u001b[0m in \u001b[0;36mget_model_config\u001b[0;34m(self, model_name, model_version, headers, as_json, client_timeout)\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrpc_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mraise_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     def get_model_repository_index(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tritonclient/grpc/_utils.py\u001b[0m in \u001b[0;36mraise_error_grpc\u001b[0;34m(rpc_error)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mInferenceServerException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mget_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:8000: Failed to connect to remote host: Connection refused"
     ]
    }
   ],
   "source": [
    "embedding_client.get_embedding(\"zhangw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38aae526-0da6-41ee-b7d3-a6ebdaa273fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'qanything_kernel/connector/embedding/embedding_model_0630'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/39x7dcks3qlg8mtfpkhrn96h0000gn/T/ipykernel_87701/113658200.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m embedding_client = EmbeddingClient(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mserver_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL_EMBED_SERVICE_URL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL_EMBED_MODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Machine/LLM/QAnything/code_analysis/../qanything_kernel/connector/embedding/embedding_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_url, model_name, model_version, tokenizer_path, resp_wait_s)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_wait_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_MAX_RESP_WAIT_S\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresp_wait_s\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresp_wait_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     ```\"\"\"\n\u001b[1;32m    485\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         ):\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'qanything_kernel/connector/embedding/embedding_model_0630'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "class YouDaoLocalEmbeddings:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _get_embedding(self, queries):\n",
    "        embeddings = embedding_client.get_embedding(queries, max_length=LOCAL_EMBED_MAX_LENGTH)\n",
    "        return embeddings\n",
    "\n",
    "    def _get_len_safe_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        batch_size = LOCAL_EMBED_BATCH\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            # 按照batch_size大小进行数据切分，限制文本长度, \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i + batch_size]\n",
    "                future = executor.submit(self._get_embedding, batch)\n",
    "                futures.append(future)\n",
    "            debug_logger.info(f'embedding number: {len(futures)}')\n",
    "            # 合并embedding\n",
    "            for future in tqdm(futures):\n",
    "                embeddings = future.result()\n",
    "                all_embeddings += embeddings\n",
    "        return all_embeddings\n",
    "\n",
    "    @property\n",
    "    def embed_version(self):\n",
    "        return embedding_client.getModelVersion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98043000-6084-4939-bab9-949d8a11d78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
