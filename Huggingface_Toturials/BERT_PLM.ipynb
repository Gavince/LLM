{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9fc3ea-fd8d-43eb-bc11-b8d3fe748b8f",
   "metadata": {},
   "source": [
    "# BERT预训练模型应用实践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68375bb-fb4b-4745-b216-951b1231838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7735e440-8ddc-4a56-af6a-355c62da9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import unicodedata\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# from .file_utils import cached_path\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ddd91-83ca-46bd-9f06-26f8e26edc4b",
   "metadata": {},
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ffde03-f139-4b92-931d-b0287cf29078",
   "metadata": {},
   "source": [
    "### 基本判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3872e019-1866-4a29-ab8e-7eef126accc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae694a6-ff09-4062-b9b3-ae98f9133574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词表加载\n",
    "PRETRAINED_VOCAB_ARCHIVE_MAP = {\n",
    "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n",
    "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n",
    "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n",
    "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n",
    "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n",
    "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n",
    "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n",
    "}\n",
    "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n",
    "    'bert-base-uncased': 512,\n",
    "    'bert-large-uncased': 512,\n",
    "    'bert-base-cased': 512,\n",
    "    'bert-large-cased': 512,\n",
    "    'bert-base-multilingual-uncased': 512,\n",
    "    'bert-base-multilingual-cased': 512,\n",
    "    'bert-base-chinese': 512,\n",
    "}\n",
    "VOCAB_NAME = 'vocab.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612676b-5408-41e8-ba08-8d1d9c6c3fe3",
   "metadata": {},
   "source": [
    "### 加载词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf5b127-1b6f-4b74-ac0f-1689b4c7825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ed8170-ac8c-4b31-bcac-9cf93ba0ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = load_vocab(\"./data/bert-base-chinese-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22bea4b9-a993-40b0-9081-e87ebb6ac106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accea617-1a54-4135-8ce8-ed10ac74c387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['你好', '在', '那', '67快看', 'hell']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词\n",
    "whitespace_tokenize(\"你好 在 那 uu快看 hell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e37def-0d2d-4f2c-a913-7d2b8a01f209",
   "metadata": {},
   "source": [
    "### Text_token\n",
    "完成文本转为token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeabe74-e535-4e98-9d6e-669dd423d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizer(object):\n",
    "    \"\"\"Runs end-to-end tokenization: punctuation splitting + wordpiece\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=True, max_len=None):\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
    "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict(\n",
    "            [(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "        self.max_len = max_len if max_len is not None else int(1e12)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "        return split_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab[token])\n",
    "        if len(ids) > self.max_len:\n",
    "            raise ValueError(\n",
    "                \"Token indices sequence length is longer than the specified maximum \"\n",
    "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
    "                \" sequence through BERT will result in indexing errors\".format(len(ids), self.max_len)\n",
    "            )\n",
    "        return ids\n",
    "\n",
    "    def convert_tokens_to_ids_no_warning(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab[token])\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens.append(self.ids_to_tokens[i])\n",
    "        return tokens\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name, cache_dir=None, *inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiate a PreTrainedBertModel from a pre-trained model file.\n",
    "        Download and cache the pre-trained model file if needed.\n",
    "        \"\"\"\n",
    "        if pretrained_model_name in PRETRAINED_VOCAB_ARCHIVE_MAP:\n",
    "            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name]\n",
    "        else:\n",
    "            vocab_file = pretrained_model_name\n",
    "        if os.path.isdir(vocab_file):\n",
    "            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n",
    "        # redirect to the cache, if necessary\n",
    "        try:\n",
    "            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(\n",
    "                \"Model name '{}' was not found in model name list ({}). \"\n",
    "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
    "                \"associated to this path or url.\".format(\n",
    "                    pretrained_model_name,\n",
    "                    ', '.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n",
    "                    vocab_file))\n",
    "            return None\n",
    "        if resolved_vocab_file == vocab_file:\n",
    "            logger.info(\"loading vocabulary file {}\".format(vocab_file))\n",
    "        else:\n",
    "            logger.info(\"loading vocabulary file {} from cache at {}\".format(\n",
    "                vocab_file, resolved_vocab_file))\n",
    "        if pretrained_model_name in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n",
    "            # if we're using a pretrained model, ensure the tokenizer wont index sequences longer\n",
    "            # than the number of positional embeddings\n",
    "            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name]\n",
    "            kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n",
    "        # Instantiate tokenizer.\n",
    "        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "582b9b15-9353-45d3-bd9a-f168f0f22285",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_token = BertTokenizer(\"./data/bert-base-chinese-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ab675e8-8f0d-484e-a47c-61dd8cb2ccf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zh', '##ang', 'h', '你', '好', '，', '的', '咋', 'd', ';', '%', 'k', '##ill']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = bert_token.tokenize(\"zhang h你好，的咋d;%kill\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02fed10c-1ba4-439c-99d4-ed3d258dc0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9998, 8688, 150, 872, 1962, 8024, 4638, 1468, 146, 132, 110, 153, 9868]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = bert_token.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d91bb922-4ab9-4e97-93a5-f98d28a98d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zh', '##ang', 'h', '你', '好', '，', '的', '咋', 'd', ';', '%', 'k', '##ill']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_token.convert_ids_to_tokens(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1096be50-eb09-4a54-966a-8a3ab9555223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer(object):\n",
    "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
    "\n",
    "    def __init__(self, do_lower_case=True):\n",
    "        \"\"\"Constructs a BasicTokenizer.\n",
    "\n",
    "        Args:\n",
    "          do_lower_case: Whether to lower case the input.\n",
    "        \"\"\"\n",
    "        self.do_lower_case = do_lower_case\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "        text = self._clean_text(text)\n",
    "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "        # models. This is also applied to the English models now, but it doesn't\n",
    "        # matter since the English models were not trained on any Chinese data\n",
    "        # and generally don't have any Chinese data in them (there are Chinese\n",
    "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
    "        # words in the English Wikipedia.).\n",
    "        text = self._tokenize_chinese_chars(text)\n",
    "        # 按照空格进行切分, eg:你好ok胡k总->[你，好，OK， 胡，k, 总]\n",
    "        orig_tokens = whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if self.do_lower_case:\n",
    "                token = token.lower()\n",
    "                token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text):\n",
    "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    def _tokenize_chinese_chars(self, text):\n",
    "        \"\"\"Adds whitespace around any CJK character.\n",
    "            在中文字符的周围加上空白的空格\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if self._is_chinese_char(cp):\n",
    "                output.append(\" \")\n",
    "                output.append(char)\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _is_chinese_char(self, cp):\n",
    "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "        #\n",
    "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "        # space-separated words, so they are not treated specially and handled\n",
    "        # like the all of the other languages.\n",
    "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbe5f45a-4c30-4bca-ad2c-b63912951002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "b_token = BasicTokenizer(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "decc1886-e76d-4617-b005-de5c60a64884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['张', '，', '你', 'okm', ',', ';', ';', 'm', '集', 'm', ';', ';']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_token.tokenize(\"张，你okm,;;m集m;;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45caec5-6f61-4ab7-9ea1-d48eb5e962cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
    "\n",
    "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
    "        using the given vocabulary.\n",
    "\n",
    "        For example:\n",
    "          input = \"unaffable\"\n",
    "          output = [\"un\", \"##aff\", \"##able\"]\n",
    "\n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer`.\n",
    "\n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                # 从后往前依次切分数据\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "                \n",
    "        return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b113f31-622f-4904-8db0-13dce604e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token = WordpieceTokenizer(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "212384c8-a1f9-4fbd-9897-46c8a56b9a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['张', '##hu', '##你', '##来', '##了', '##吗', '##ok']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token.tokenize(\"张hu你来了吗ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82301a-05ca-4211-8d56-17c4d33726ab",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47a94721-ccf0-4408-aaae-c0823b73ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Modified by Harold. Added VisualBERT.\n",
    "\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"PyTorch BERT model.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import tarfile\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# from .file_utils import cached_path\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
    "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n",
    "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n",
    "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n",
    "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n",
    "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n",
    "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n",
    "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n",
    "}\n",
    "CONFIG_NAME = 'bert_config.json'\n",
    "WEIGHTS_NAME = 'pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361417cd-a6c5-400c-aeb6-c4bf64ca9262",
   "metadata": {},
   "source": [
    "### 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6e128df-fe3f-4158-9bb7-279ef3adc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Implementation of the gelu activation function.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0844ad-042a-48a8-93d7-a912b1c42cd9",
   "metadata": {},
   "source": [
    "### BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "495bdee8-2a5b-43e3-a1a7-2e0ab5d458e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(object):\n",
    "    \"\"\"Configuration class to store the configuration of a `BertModel`.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size_or_config_json_file,\n",
    "                 hidden_size=768,\n",
    "                 num_hidden_layers=12,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=2,\n",
    "                 initializer_range=0.02):\n",
    "        \"\"\"Constructs BertConfig.\n",
    "\n",
    "        Args:\n",
    "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n",
    "            hidden_size: Size of the encoder layers and the pooler layer.\n",
    "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
    "            num_attention_heads: Number of attention heads for each attention layer in\n",
    "                the Transformer encoder.\n",
    "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
    "                layer in the Transformer encoder.\n",
    "            hidden_act: The non-linear activation function (function or string) in the\n",
    "                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
    "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
    "                layers in the embeddings, encoder, and pooler.\n",
    "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
    "                probabilities.\n",
    "            max_position_embeddings: The maximum sequence length that this model might\n",
    "                ever be used with. Typically set this to something large just in case\n",
    "                (e.g., 512 or 1024 or 2048).\n",
    "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
    "                `BertModel`.\n",
    "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
    "                initializing all weight matrices.\n",
    "        \"\"\"\n",
    "        if isinstance(vocab_size_or_config_json_file, str):\n",
    "            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n",
    "                json_config = json.loads(reader.read())\n",
    "            for key, value in json_config.items():\n",
    "                self.__dict__[key] = value\n",
    "        elif isinstance(vocab_size_or_config_json_file, int):\n",
    "            self.vocab_size = vocab_size_or_config_json_file\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_hidden_layers = num_hidden_layers\n",
    "            self.num_attention_heads = num_attention_heads\n",
    "            self.hidden_act = hidden_act\n",
    "            self.intermediate_size = intermediate_size\n",
    "            self.hidden_dropout_prob = hidden_dropout_prob\n",
    "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "            self.max_position_embeddings = max_position_embeddings\n",
    "            self.type_vocab_size = type_vocab_size\n",
    "            self.initializer_range = initializer_range\n",
    "        else:\n",
    "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
    "                             \"or the path to a pretrained model config file (str)\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, json_object):\n",
    "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
    "        config = BertConfig(vocab_size_or_config_json_file=-1)\n",
    "        for key, value in json_object.items():\n",
    "            config.__dict__[key] = value\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_json_file(cls, json_file):\n",
    "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
    "        with open(json_file, \"r\", encoding='utf-8') as reader:\n",
    "            text = reader.read()\n",
    "        return cls.from_dict(json.loads(text))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33f7e4-f077-45ae-9230-1aa1c81ef86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "382c76a9-df20-410f-90c8-8a9cc9bb543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\n",
    "except ImportError:\n",
    "    print(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\")\n",
    "    class BertLayerNorm(nn.Module):\n",
    "        def __init__(self, hidden_size, eps=1e-12):\n",
    "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "            \"\"\"\n",
    "            super(BertLayerNorm, self).__init__()\n",
    "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "            self.variance_epsilon = eps\n",
    "\n",
    "        def forward(self, x):\n",
    "            u = x.mean(-1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "            return self.weight * x + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c893880-1171-42f2-9a94-a382ff9c387c",
   "metadata": {},
   "source": [
    "### BERT Embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8772be4-0db6-408d-9139-aaa5230e3a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5294a-f22b-416e-8299-b31afb472edd",
   "metadata": {},
   "source": [
    "### BertSelfAttention\n",
    "单个头的自注意力mo xin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c49b9ac-1a24-45a6-89bd-7d07a0f1900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "        self.output_attention_weights = config.output_attention_weights\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        if self.output_attention_weights:\n",
    "            return context_layer, attention_probs\n",
    "        else:\n",
    "            return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d251fda9-672c-4402-a08e-6c8d341041de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9be02c4-d60e-48e5-939e-87b3f49576ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "        self.output_attention_weights = config.output_attention_weights\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        if self.output_attention_weights:\n",
    "            self_output, attention_weights = self.self(input_tensor, attention_mask)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output, attention_weights\n",
    "        else:\n",
    "            self_output = self.self(input_tensor, attention_mask)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab1ad5-a0b1-467f-bf5e-59dcd01c1647",
   "metadata": {},
   "source": [
    "### FFN层设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "670af7c1-340d-434a-a09e-77adc5f82641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_act_fn = ACT2FN[config.hidden_act] \\\n",
    "            if isinstance(config.hidden_act, str) else config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9964b5f-4ac1-4b62-9b6a-ce164a10e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a00fd9a-44ad-436b-a2f8-d4eea1ebe705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "        self.output_attention_weights = config.output_attention_weights\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        if self.output_attention_weights:\n",
    "            attention_output, attention_weights = self.attention(hidden_states, attention_mask)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            return layer_output, attention_weights\n",
    "        else:\n",
    "            attention_output = self.attention(hidden_states, attention_mask)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f11b6ea-6718-44cc-8180-95bb8733be80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/39x7dcks3qlg8mtfpkhrn96h0000gn/T/ipykernel_32943/2130866606.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mACT2FN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"gelu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"swish\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mswish\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        layer = BertLayer(config)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
    "        self.output_attention_weights = config.output_attention_weights\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
    "        if self.output_attention_weights:\n",
    "            attn_data_list = []\n",
    "            all_encoder_layers = []\n",
    "            for layer_module in self.layer:\n",
    "                hidden_states, attention_weights = layer_module(hidden_states, attention_mask)\n",
    "                if output_all_encoded_layers:\n",
    "                    all_encoder_layers.append(hidden_states)\n",
    "                attn_data_list.append(attention_weights)\n",
    "            if not output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "            return all_encoder_layers, attn_data_list\n",
    "        else:\n",
    "            all_encoder_layers = []\n",
    "            for layer_module in self.layer:\n",
    "                hidden_states = layer_module(hidden_states, attention_mask)\n",
    "                if output_all_encoded_layers:\n",
    "                    all_encoder_layers.append(hidden_states)\n",
    "            if not output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "            return all_encoder_layers\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPredictionHeadTransform, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.transform_act_fn = ACT2FN[config.hidden_act] \\\n",
    "            if isinstance(config.hidden_act, str) else config.hidden_act\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config, bert_model_embedding_weights):\n",
    "        super(BertLMPredictionHead, self).__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n",
    "                                 bert_model_embedding_weights.size(0),\n",
    "                                 bias=False)\n",
    "        self.decoder.weight = bert_model_embedding_weights\n",
    "        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config, bert_model_embedding_weights):\n",
    "        super(BertOnlyMLMHead, self).__init__()\n",
    "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "\n",
    "\n",
    "class BertOnlyNSPHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertOnlyNSPHead, self).__init__()\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return seq_relationship_score\n",
    "\n",
    "\n",
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config, bert_model_embedding_weights):\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score\n",
    "\n",
    "\n",
    "class PreTrainedBertModel(nn.Module):\n",
    "    \"\"\" An abstract class to handle weights initialization and\n",
    "        a simple interface for dowloading and loading pretrained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(PreTrainedBertModel, self).__init__()\n",
    "        if not isinstance(config, BertConfig):\n",
    "            raise ValueError(\n",
    "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
    "                \"To create a model from a Google pretrained model use \"\n",
    "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
    "                    self.__class__.__name__, self.__class__.__name__\n",
    "                ))\n",
    "        self.config = config\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name, state_dict=None, cache_dir=None, random_initialize = False, *inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict.\n",
    "        Download and cache the pre-trained model file if needed.\n",
    "\n",
    "        Params:\n",
    "            pretrained_model_name: either:\n",
    "                - a str with the name of a pre-trained model to load selected in the list of:\n",
    "                    . `bert-base-uncased`\n",
    "                    . `bert-large-uncased`\n",
    "                    . `bert-base-cased`\n",
    "                    . `bert-large-cased`\n",
    "                    . `bert-base-multilingual-uncased`\n",
    "                    . `bert-base-multilingual-cased`\n",
    "                    . `bert-base-chinese`\n",
    "                - a path or url to a pretrained model archive containing:\n",
    "                    . `bert_config.json` a configuration file for the model\n",
    "                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
    "            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n",
    "            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n",
    "            *inputs, **kwargs: additional input for the specific Bert class\n",
    "                (ex: num_labels for BertForSequenceClassification)\n",
    "        \"\"\"\n",
    "        if pretrained_model_name in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
    "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name]\n",
    "        else:\n",
    "            archive_file = pretrained_model_name\n",
    "        # redirect to the cache, if necessary\n",
    "        try:\n",
    "            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(\n",
    "                \"Model name '{}' was not found in model name list ({}). \"\n",
    "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
    "                \"associated to this path or url.\".format(\n",
    "                    pretrained_model_name,\n",
    "                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
    "                    archive_file))\n",
    "            return None\n",
    "        if resolved_archive_file == archive_file:\n",
    "            logger.info(\"loading archive file {}\".format(archive_file))\n",
    "        else:\n",
    "            logger.info(\"loading archive file {} from cache at {}\".format(\n",
    "                archive_file, resolved_archive_file))\n",
    "        tempdir = None\n",
    "        if os.path.isdir(resolved_archive_file):\n",
    "            serialization_dir = resolved_archive_file\n",
    "        else:\n",
    "            # Extract archive to temp dir\n",
    "            tempdir = tempfile.mkdtemp()\n",
    "            logger.info(\"extracting archive file {} to temp dir {}\".format(\n",
    "                resolved_archive_file, tempdir))\n",
    "            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
    "                archive.extractall(tempdir)\n",
    "            serialization_dir = tempdir\n",
    "        # Load config\n",
    "        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
    "        config = BertConfig.from_json_file(config_file)\n",
    "        logger.info(\"Model config {}\".format(config))\n",
    "        # Instantiate model.\n",
    "        model = cls(config, *inputs, **kwargs)\n",
    "\n",
    "        if random_initialize:\n",
    "            return model\n",
    "\n",
    "        if state_dict is None:\n",
    "            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
    "            state_dict = torch.load(weights_path)\n",
    "\n",
    "        old_keys = []\n",
    "        new_keys = []\n",
    "        for key in state_dict.keys():\n",
    "            new_key = None\n",
    "            if 'gamma' in key:\n",
    "                new_key = key.replace('gamma', 'weight')\n",
    "            if 'beta' in key:\n",
    "                new_key = key.replace('beta', 'bias')\n",
    "            if new_key:\n",
    "                old_keys.append(key)\n",
    "                new_keys.append(new_key)\n",
    "        for old_key, new_key in zip(old_keys, new_keys):\n",
    "            state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        missing_keys = []\n",
    "        unexpected_keys = []\n",
    "        error_msgs = []\n",
    "        # copy state_dict so _load_from_state_dict can modify it\n",
    "        metadata = getattr(state_dict, '_metadata', None)\n",
    "        state_dict = state_dict.copy()\n",
    "        if metadata is not None:\n",
    "            state_dict._metadata = metadata\n",
    "\n",
    "        def load(module, prefix=''):\n",
    "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "            module._load_from_state_dict(\n",
    "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "            for name, child in module._modules.items():\n",
    "                if child is not None:\n",
    "                    load(child, prefix + name + '.')\n",
    "        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')\n",
    "        if len(missing_keys) > 0:\n",
    "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
    "                model.__class__.__name__, missing_keys))\n",
    "        if len(unexpected_keys) > 0:\n",
    "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
    "                model.__class__.__name__, unexpected_keys))\n",
    "        if tempdir:\n",
    "            # Clean up temp dir\n",
    "            shutil.rmtree(tempdir)\n",
    "        return model\n",
    "\n",
    "\n",
    "class BertModel(PreTrainedBertModel):\n",
    "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
    "\n",
    "    Params:\n",
    "        config: a BertConfig class instance with the configuration to build a new model\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
    "\n",
    "    Outputs: Tuple of (encoded_layers, pooled_output)\n",
    "        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
    "            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
    "                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
    "                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
    "            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
    "                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
    "        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
    "            classifier pretrained on top of the hidden state associated to the first character of the\n",
    "            input (`CLF`) to train on the Next-Sentence task (see BERT's paper).\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    model = modeling.BertModel(config=config)\n",
    "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask,\n",
    "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "class BertForPreTraining(PreTrainedBertModel):\n",
    "    \"\"\"BERT model with pre-training heads.\n",
    "    This module comprises the BERT model followed by the two pre-training heads:\n",
    "        - the masked language modeling head, and\n",
    "        - the next sentence classification head.\n",
    "\n",
    "    Params:\n",
    "        config: a BertConfig class instance with the configuration to build a new model.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n",
    "            is only computed for the labels set in [0, ..., vocab_size]\n",
    "        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, 1].\n",
    "            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n",
    "\n",
    "    Outputs:\n",
    "        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n",
    "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
    "            sentence classification loss.\n",
    "        if `masked_lm_labels` or `next_sentence_label` is `None`:\n",
    "            Outputs a tuple comprising\n",
    "            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n",
    "            - the next sentence classification logits of shape [batch_size, 2].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    model = BertForPreTraining(config)\n",
    "    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForPreTraining, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n",
    "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                                   output_all_encoded_layers=False)\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        if masked_lm_labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "            return total_loss\n",
    "        elif masked_lm_labels is not None and next_sentence_label is None: # If we did not specify the next_sentence_label\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
    "            total_loss = masked_lm_loss\n",
    "            return total_loss\n",
    "        else:\n",
    "            return prediction_scores, seq_relationship_score\n",
    "\n",
    "\n",
    "class BertForMaskedLM(PreTrainedBertModel):\n",
    "    \"\"\"BERT model with the masked language modeling head.\n",
    "    This module comprises the BERT model followed by the masked language modeling head.\n",
    "\n",
    "    Params:\n",
    "        config: a BertConfig class instance with the configuration to build a new model.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n",
    "            is only computed for the labels set in [0, ..., vocab_size]\n",
    "\n",
    "    Outputs:\n",
    "        if `masked_lm_labels` is  not `None`:\n",
    "            Outputs the masked language modeling loss.\n",
    "        if `masked_lm_labels` is `None`:\n",
    "            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    model = BertForMaskedLM(config)\n",
    "    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForMaskedLM, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n",
    "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                       output_all_encoded_layers=False)\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        if masked_lm_labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
    "            return masked_lm_loss\n",
    "        else:\n",
    "            return prediction_scores\n",
    "\n",
    "\n",
    "class BertForNextSentencePrediction(PreTrainedBertModel):\n",
    "    \"\"\"BERT model with next sentence prediction head.\n",
    "    This module comprises the BERT model followed by the next sentence classification head.\n",
    "\n",
    "    Params:\n",
    "        config: a BertConfig class instance with the configuration to build a new model.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, 1].\n",
    "            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n",
    "\n",
    "    Outputs:\n",
    "        if `next_sentence_label` is not `None`:\n",
    "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
    "            sentence classification loss.\n",
    "        if `next_sentence_label` is `None`:\n",
    "            Outputs the next sentence classification logits of shape [batch_size, 2].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    model = BertForNextSentencePrediction(config)\n",
    "    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForNextSentencePrediction, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertOnlyNSPHead(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        seq_relationship_score = self.cls( pooled_output)\n",
    "\n",
    "        if next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            return next_sentence_loss\n",
    "        else:\n",
    "            return seq_relationship_score\n",
    "\n",
    "\n",
    "class BertForSequenceClassification(PreTrainedBertModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    num_labels = 2\n",
    "\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "class BertForMultipleChoice(PreTrainedBertModel):\n",
    "    \"\"\"BERT model for multiple choice tasks.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_choices`: the number of classes for the classifier. Default = 2.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n",
    "            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n",
    "            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_choices].\n",
    "\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n",
    "    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n",
    "    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    num_choices = 2\n",
    "\n",
    "    model = BertForMultipleChoice(config, num_choices)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_choices=2):\n",
    "        super(BertForMultipleChoice, self).__init__(config)\n",
    "        self.num_choices = num_choices\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n",
    "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n",
    "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
    "        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        reshaped_logits = logits.view(-1, self.num_choices)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(reshaped_logits, labels)\n",
    "            return loss\n",
    "        else:\n",
    "            return reshaped_logits\n",
    "\n",
    "\n",
    "class BertForTokenClassification(PreTrainedBertModel):\n",
    "    \"\"\"BERT model for token-level classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the full hidden state of the last layer.\n",
    "\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    num_labels = 2\n",
    "\n",
    "    model = BertForTokenClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForTokenClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "class BertForQuestionAnswering(PreTrainedBertModel):\n",
    "    \"\"\"BERT model for Question Answering (span extraction).\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the sequence output that computes start_logits and end_logits\n",
    "\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n",
    "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
    "            into account for computing the loss.\n",
    "        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n",
    "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
    "            into account for computing the loss.\n",
    "\n",
    "    Outputs:\n",
    "        if `start_positions` and `end_positions` are not `None`:\n",
    "            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n",
    "        if `start_positions` or `end_positions` is `None`:\n",
    "            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n",
    "            position tokens of shape [batch_size, sequence_length].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    model = BertForQuestionAnswering(config)\n",
    "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForQuestionAnswering, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n",
    "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n",
    "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            return total_loss\n",
    "        else:\n",
    "            return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016c35e-b704-4a04-aba6-2efb7ce64ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#### Added by Harold ####\n",
    "#########################\n",
    "\n",
    "class BertEmbeddingsWithVisualEmbedding(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position, token_type embeddings and visual embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddingsWithVisualEmbedding, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        #### Below are specific for encoding visual features\n",
    "\n",
    "        # Segment and position embedding for image features\n",
    "        self.token_type_embeddings_visual = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.position_embeddings_visual = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "\n",
    "        self.projection = nn.Linear(config.visual_embedding_dim, config.hidden_size)\n",
    "\n",
    "    def special_intialize(self, method_type = 0):\n",
    "        ### This is a bit unorthodox. The better way might be to add an inititilizer to AllenNLP.\n",
    "        # This function is used to initialize the token_type_embeddings_visual and positiona_embedding_visual, just incase.\n",
    "        self.token_type_embeddings_visual.weight = torch.nn.Parameter(deepcopy(self.token_type_embeddings.weight.data), requires_grad = True)\n",
    "        self.position_embeddings_visual.weight = torch.nn.Parameter(deepcopy(self.position_embeddings.weight.data), requires_grad = True)\n",
    "        return\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, visual_embeddings=None, visual_embeddings_type=None, position_embeddings_visual=None, image_text_alignment = None, confidence = None):\n",
    "        '''\n",
    "        input_ids = [batch_size, sequence_length]\n",
    "        token_type_ids = [batch_size, sequence_length]\n",
    "        visual_embedding = [batch_size, image_feature_length, image_feature_dim]\n",
    "        image_text_alignment = [batch_size, image_feature_length, alignment_dim]\n",
    "        confidence = [batch_size, image_feature_length] of type LongTensor\n",
    "        '''\n",
    "\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "\n",
    "        if visual_embeddings is not None:\n",
    "            visual_embeddings = self.projection(visual_embeddings)\n",
    "            token_type_embeddings_visual = self.token_type_embeddings_visual(visual_embeddings_type)\n",
    "\n",
    "            if image_text_alignment is not None:\n",
    "                # image_text_alignment = Batch x image_length x alignment_number. Each element denotes the position of the word corresponding to the image feature. -1 is the padding value.\n",
    "                image_text_alignment_mask = (image_text_alignment != -1).long()\n",
    "                # Get rid of the -1.\n",
    "                image_text_alignment = image_text_alignment_mask * image_text_alignment\n",
    "\n",
    "                # position_embeddings_visual = Batch x image_length x alignment length x dim\n",
    "                position_embeddings_visual = self.position_embeddings(image_text_alignment) * image_text_alignment_mask.to(dtype=next(self.parameters()).dtype).unsqueeze(-1)\n",
    "                position_embeddings_visual = position_embeddings_visual.sum(2)\n",
    "\n",
    "                # We want to averge along the alignment_number dimension.\n",
    "                image_text_alignment_mask = image_text_alignment_mask.to(dtype=next(self.parameters()).dtype).sum(2)\n",
    "                image_text_alignment_mask[image_text_alignment_mask==0] = 1 # Avoid devide by zero error\n",
    "                position_embeddings_visual = position_embeddings_visual / image_text_alignment_mask.unsqueeze(-1)\n",
    "\n",
    "                position_ids_visual = torch.zeros(*visual_embeddings.size()[:-1], dtype = torch.long).cuda()\n",
    "\n",
    "                # When fine-tuning the detector , the image_text_alignment is sometimes padded too long. \n",
    "                if position_embeddings_visual.size(1) != visual_embeddings.size(1):\n",
    "                    assert(position_embeddings_visual.size(1) >= visual_embeddings.size(1))\n",
    "                    position_embeddings_visual = position_embeddings_visual[:, :visual_embeddings.size(1), :]\n",
    "\n",
    "                position_embeddings_visual = position_embeddings_visual + self.position_embeddings_visual(position_ids_visual)\n",
    "            else:\n",
    "                position_ids_visual = torch.zeros(*visual_embeddings.size()[:-1], dtype = torch.long).cuda()\n",
    "                position_embeddings_visual = self.position_embeddings_visual(position_ids_visual)\n",
    "\n",
    "            v_embeddings = visual_embeddings + position_embeddings_visual + token_type_embeddings_visual\n",
    "\n",
    "            # Concate the two:\n",
    "            embeddings = torch.cat((embeddings, v_embeddings), dim = 1) # concat the visual embeddings after the attentions\n",
    "\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class BertVisualModel(PreTrainedBertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertVisualModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddingsWithVisualEmbedding(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.bypass_transformer = config.bypass_transformer\n",
    "\n",
    "        if self.bypass_transformer:\n",
    "            self.additional_layer = BertLayer(config)   \n",
    "\n",
    "        self.output_attention_weights = config.output_attention_weights         \n",
    "\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, visual_embeddings, position_embeddings_visual, visual_embeddings_type, image_text_alignment, confidence, output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids, visual_embeddings = visual_embeddings, position_embeddings_visual = position_embeddings_visual, visual_embeddings_type = visual_embeddings_type, image_text_alignment = image_text_alignment,\n",
    "            confidence = confidence)\n",
    "\n",
    "        if self.bypass_transformer and visual_embeddings is not None:\n",
    "            assert(not output_all_encoded_layers) # Don't support this for the bypass model\n",
    "            text_length = input_ids.size(1)\n",
    "            text_embedding_output = embedding_output[:, :text_length, :]\n",
    "            visual_part = embedding_output[:, text_length:, :]\n",
    "\n",
    "            text_extended_attention_mask = extended_attention_mask[:, :, :text_length, :text_length]\n",
    "\n",
    "            encoded_layers = self.encoder(text_embedding_output,\n",
    "                                      text_extended_attention_mask,\n",
    "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
    "            sequence_output = encoded_layers[-1]\n",
    "            new_input = torch.cat((sequence_output, visual_part), dim = 1)\n",
    "            final_sequence_output = self.additional_layer(new_input, extended_attention_mask)\n",
    "            pooled_output = self.pooler(final_sequence_output)\n",
    "            return final_sequence_output, pooled_output\n",
    "\n",
    "        if self.output_attention_weights:\n",
    "            encoded_layers, attn_data_list = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers=output_all_encoded_layers)\n",
    "            sequence_output = encoded_layers[-1]\n",
    "            pooled_output = self.pooler(sequence_output)\n",
    "            if not output_all_encoded_layers:\n",
    "                encoded_layers = encoded_layers[-1]\n",
    "            return encoded_layers, pooled_output, attn_data_list\n",
    "        else:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers=output_all_encoded_layers)\n",
    "            sequence_output = encoded_layers[-1]\n",
    "            pooled_output = self.pooler(sequence_output)\n",
    "            if not output_all_encoded_layers:\n",
    "                encoded_layers = encoded_layers[-1]\n",
    "            return encoded_layers, pooled_output\n",
    "\n",
    "class TrainVisualBERTObjective(PreTrainedBertModel):\n",
    "    def __init__(self, config, training_head_type, visual_embedding_dim = 512, hard_cap_seq_len = None, cut_first = \"text\", embedding_strategy = \"plain\", bypass_transformer = False, output_attention_weights= False):\n",
    "        super(TrainVisualBERTObjective, self).__init__(config)\n",
    "        config.visual_embedding_dim = visual_embedding_dim\n",
    "\n",
    "        config.embedding_strategy = embedding_strategy\n",
    "\n",
    "        config.bypass_transformer = bypass_transformer\n",
    "\n",
    "        config.output_attention_weights = output_attention_weights  \n",
    "        self.output_attention_weights = output_attention_weights  \n",
    "\n",
    "        self.cut_first = cut_first\n",
    "        self.hard_cap_seq_len = hard_cap_seq_len\n",
    "        self.bert = BertVisualModel(config)\n",
    "\n",
    "        self.training_head_type = training_head_type\n",
    "        if self.training_head_type == \"pretraining\":\n",
    "            self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        elif self.training_head_type == \"multichoice\":\n",
    "            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "            self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "            self.num_choices = 4 # For VCR\n",
    "\n",
    "        elif self.training_head_type == \"vqa\":\n",
    "            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "            self.classifier = nn.Linear(config.hidden_size, 3129)\n",
    "        elif self.training_head_type == \"vqa_advanced\":\n",
    "            self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        elif self.training_head_type == \"nlvr\":\n",
    "            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "            self.classifier = nn.Linear(config.hidden_size, 2)\n",
    "        elif self.training_head_type == \"flickr\":\n",
    "            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "            self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "            self.flickr_attention = FlickrAttention(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        token_type_ids, \n",
    "        input_mask,\n",
    "\n",
    "        visual_embeddings,\n",
    "        position_embeddings_visual, \n",
    "        image_mask, \n",
    "        image_text_alignment = None,\n",
    "        confidence = None,\n",
    "\n",
    "        visual_embeddings_type=None,\n",
    "        label=None,\n",
    "        flickr_position = None,\n",
    "        masked_lm_labels=None,\n",
    "        image_lm_lables=None,\n",
    "        is_random_next=None,\n",
    "\n",
    "        output_all_encoded_layers = False):\n",
    "\n",
    "        # We want to convert everything into: batch x sequence_length x (dim).\n",
    "\n",
    "        flat_input_ids = transform_to_batch_sequence(input_ids)\n",
    "        flat_token_type_ids = transform_to_batch_sequence(token_type_ids)\n",
    "        flat_input_mask = transform_to_batch_sequence(input_mask)\n",
    "\n",
    "        flat_image_mask = transform_to_batch_sequence(image_mask)\n",
    "        flat_masked_lm_labels = transform_to_batch_sequence(masked_lm_labels)\n",
    "        flat_position_embeddings_visual = transform_to_batch_sequence(position_embeddings_visual)\n",
    "        flat_confidence = transform_to_batch_sequence(confidence)\n",
    "\n",
    "        flat_image_text_alignment = transform_to_batch_sequence_dim(image_text_alignment)\n",
    "        flat_visual_embeddings = transform_to_batch_sequence_dim(visual_embeddings)\n",
    "\n",
    "        if visual_embeddings_type is not None:\n",
    "            visual_embeddings_type = transform_to_batch_sequence(visual_embeddings_type)\n",
    "        else:\n",
    "            if flat_image_mask is not None:\n",
    "                visual_embeddings_type = torch.zeros_like(flat_image_mask, dtype = torch.long)\n",
    "            else:\n",
    "                visual_embeddings_type = None\n",
    "\n",
    "        if flat_image_mask is not None:\n",
    "            flat_attention_mask = torch.cat((flat_input_mask, flat_image_mask), dim = -1)\n",
    "\n",
    "            assert(image_lm_lables is None) # Do not support this yet\n",
    "            if flat_masked_lm_labels is not None:\n",
    "                assert(flat_masked_lm_labels.size(-1) == flat_input_mask.size(-1))\n",
    "                new_lm_labels = torch.ones_like(flat_attention_mask) * -1\n",
    "                size_masked_lm_labels = flat_masked_lm_labels.size()\n",
    "                assert(len(size_masked_lm_labels) == 2)\n",
    "                new_lm_labels[:size_masked_lm_labels[0], :size_masked_lm_labels[1]] = flat_masked_lm_labels\n",
    "                flat_masked_lm_labels = new_lm_labels\n",
    "        else:\n",
    "            flat_attention_mask = flat_input_mask\n",
    "\n",
    "        if self.output_attention_weights:\n",
    "            sequence_output, pooled_output, attention_weights = self.bert(\n",
    "            flat_input_ids, \n",
    "            flat_token_type_ids, \n",
    "            flat_attention_mask,\n",
    "            visual_embeddings = flat_visual_embeddings, \n",
    "            position_embeddings_visual = flat_position_embeddings_visual, \n",
    "            visual_embeddings_type = visual_embeddings_type,\n",
    "            image_text_alignment = flat_image_text_alignment,\n",
    "            confidence = flat_confidence,\n",
    "            output_all_encoded_layers=output_all_encoded_layers)\n",
    "            output_dict = {}\n",
    "            output_dict[\"attention_weights\"] = attention_weights\n",
    "            output_dict['loss'] = None\n",
    "            return output_dict\n",
    "\n",
    "        sequence_output, pooled_output = self.bert(\n",
    "            flat_input_ids, \n",
    "            flat_token_type_ids, \n",
    "            flat_attention_mask,\n",
    "            visual_embeddings = flat_visual_embeddings, \n",
    "            position_embeddings_visual = flat_position_embeddings_visual, \n",
    "            visual_embeddings_type = visual_embeddings_type,\n",
    "            image_text_alignment = flat_image_text_alignment,\n",
    "            confidence = flat_confidence,\n",
    "            output_all_encoded_layers=output_all_encoded_layers)\n",
    "\n",
    "        output_dict = {}\n",
    "\n",
    "        if output_all_encoded_layers:\n",
    "            output_dict[\"sequence_output\"] = sequence_output\n",
    "            output_dict[\"pooled_output\"] = pooled_output\n",
    "            output_dict[\"loss\"] = None\n",
    "            return output_dict\n",
    "\n",
    "        if self.training_head_type == \"pretraining\":\n",
    "            prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "            output_dict[\"logits\"] = prediction_scores\n",
    "            output_dict[\"seq_relationship_score\"] = seq_relationship_score\n",
    "            output_dict[\"loss\"] = None\n",
    "\n",
    "            if flat_masked_lm_labels is not None and is_random_next is not None:\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "                masked_lm_loss = loss_fct(prediction_scores.contiguous().view(-1, self.config.vocab_size), flat_masked_lm_labels.contiguous().view(-1))\n",
    "                next_sentence_loss = loss_fct(seq_relationship_score.contiguous().view(-1, 2), is_random_next.contiguous().view(-1))\n",
    "                output_dict[\"next_sentence_loss\"] = next_sentence_loss\n",
    "                output_dict[\"masked_lm_loss\"] = masked_lm_loss\n",
    "                output_dict[\"loss\"] = masked_lm_loss + next_sentence_loss\n",
    "            \n",
    "            if flat_masked_lm_labels is not None and is_random_next is None:\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "                masked_lm_loss = loss_fct(prediction_scores.contiguous().view(-1, self.config.vocab_size), flat_masked_lm_labels.contiguous().view(-1))\n",
    "                #output_dict[\"next_sentence_loss\"] = None\n",
    "                output_dict[\"masked_lm_loss\"] = masked_lm_loss\n",
    "                output_dict[\"loss\"] = masked_lm_loss\n",
    "\n",
    "            return output_dict\n",
    "\n",
    "        elif self.training_head_type == \"multichoice\":\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "            logits = self.classifier(pooled_output)\n",
    "            reshaped_logits = logits.contiguous().view(-1, self.num_choices)\n",
    "\n",
    "            output_dict[\"logits\"] = reshaped_logits\n",
    "            output_dict[\"loss\"] = None\n",
    "\n",
    "            if label is not None:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                output_dict[\"loss\"] = loss_fct(reshaped_logits, label.contiguous())\n",
    "\n",
    "            return output_dict\n",
    "\n",
    "        elif self.training_head_type == \"vqa\":\n",
    "            index_to_gather = flat_input_mask.sum(1) - 2\n",
    "\n",
    "            pooled_output = torch.gather(sequence_output, 1, index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, sequence_output.size(-1)))\n",
    "\n",
    "            flat_input_ids = torch.gather(flat_input_ids, 1, index_to_gather.unsqueeze(-1).expand(index_to_gather.size(0), 1))\n",
    "\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "            logits = self.classifier(pooled_output)\n",
    "            reshaped_logits = logits.contiguous().view(-1, 3129)\n",
    "\n",
    "            output_dict[\"logits\"] = logits\n",
    "            output_dict[\"loss\"] = None\n",
    "            output_dict[\"accuracy\"] = None\n",
    "\n",
    "            if label is not None:\n",
    "                loss_fct = torch.nn.KLDivLoss(reduction = \"batchmean\")\n",
    "                log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "                reshaped_logits = log_softmax(reshaped_logits)\n",
    "                output_dict[\"loss\"] = loss_fct(reshaped_logits, label.contiguous())\n",
    "\n",
    "                output_dict[\"accuracy\"] = torch.sum(compute_score_with_logits(reshaped_logits, label)) / label.size(0)\n",
    "\n",
    "            return output_dict\n",
    "\n",
    "        elif self.training_head_type == \"vqa_advanced\":\n",
    "            prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "            output_dict[\"logits\"] = prediction_scores\n",
    "            output_dict[\"seq_relationship_score\"] = seq_relationship_score\n",
    "            output_dict[\"loss\"] = None\n",
    "            \n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "            masked_lm_loss = loss_fct(prediction_scores.contiguous().view(-1, self.config.vocab_size), flat_masked_lm_labels.contiguous().view(-1))\n",
    "            output_dict[\"masked_lm_loss\"] = masked_lm_loss\n",
    "            output_dict[\"loss\"] = masked_lm_loss\n",
    "\n",
    "            prediction_tokens = torch.max(prediction_scores, -1)[1].view(input_ids.size(0), -1).cpu().numpy() # batch x sequence length , records the predicted words\n",
    "            lm_labels = flat_masked_lm_labels.view(input_ids.size(0), -1).cpu().numpy()\n",
    "            counter = 0.0\n",
    "            flags = []\n",
    "            for i in range(lm_labels.shape[0]):\n",
    "                flag = True\n",
    "                for j in range(lm_labels.shape[1]):\n",
    "                    if lm_labels[i][j] != -1 and prediction_tokens[i][j] != lm_labels[i][j]:\n",
    "                        flag = False\n",
    "                        break\n",
    "                if flag:\n",
    "                    counter += 1\n",
    "                flags.append(flag)\n",
    "\n",
    "            output_dict[\"accuracy\"] = counter / prediction_tokens.shape[0]\n",
    "\n",
    "            return output_dict\n",
    "\n",
    "        elif self.training_head_type == \"nlvr\":\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "            logits = self.classifier(pooled_output)\n",
    "            reshaped_logits = logits.contiguous()\n",
    "\n",
    "            output_dict[\"logits\"] = logits\n",
    "            output_dict[\"loss\"] = None\n",
    "            if label is not None:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                output_dict[\"loss\"] = loss_fct(reshaped_logits, label.contiguous())\n",
    "            return output_dict\n",
    "\n",
    "        elif self.training_head_type == \"flickr\":\n",
    "            if flickr_position is not None:\n",
    "                entities_num = (flickr_position != -1).long().view(-1).sum(-1)\n",
    "                flickr_position_mask = (flickr_position != -1).long()\n",
    "\n",
    "                # Make the -1 become 0\n",
    "                flickr_position = flickr_position * flickr_position_mask\n",
    "\n",
    "                # Selected_positions = batch x selected position x dim\n",
    "                selected_positions = batched_index_select(sequence_output, 1, flickr_position)\n",
    "\n",
    "                # Visual Features = batch x visual_feature_length x dim\n",
    "                visual_features = sequence_output[:, flat_input_mask.size(1): ,:]\n",
    "                assert(visual_features.size(1) == flat_image_mask.size(1))\n",
    "\n",
    "                scores = self.flickr_attention(selected_positions, visual_features, flat_image_mask)\n",
    "\n",
    "                # scores = batch x selected position x visual_feature\n",
    "                # scores = selected_positions.bmm(visual_features.transpose(1,2))\n",
    "                loss_fct = torch.nn.KLDivLoss(reduction = \"batchmean\")\n",
    "                log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "                scores = log_softmax(scores)\n",
    "\n",
    "                label = label.contiguous()\n",
    "                # label = batch x selected_postion x needed position\n",
    "                output_dict[\"loss\"] = loss_fct(scores, label)\n",
    "                acc, upper_acc = compute_score_with_logits_flickr(scores, label)\n",
    "                output_dict[\"accuracy\"] = acc / entities_num\n",
    "                output_dict[\"upperbound_accuracy\"] = upper_acc / entities_num\n",
    "                output_dict[\"entity_num\"] = entities_num\n",
    "            return output_dict\n",
    "\n",
    "\n",
    "\n",
    "class FlickrAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FlickrAttention, self).__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
    "        self.num_attention_heads = 1#config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, query, key, attention_mask):\n",
    "        attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype)\n",
    "        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "\n",
    "        mixed_query_layer = self.query(query)\n",
    "        mixed_key_layer = self.key(key)\n",
    "        # We don't need value layers\n",
    "        #mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        #value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_scores = attention_scores.squeeze(1)\n",
    "        return attention_scores\n",
    "\n",
    "def compute_score_with_logits_flickr(logits, labels, recall = 1):\n",
    "    # Manually changed the recall here when evaluating... A bit clumsy\n",
    "\n",
    "    labels_mask = (labels != 0.0).float()\n",
    "    upper_bound_labels = labels.sum(-1).view(-1).sum(-1)\n",
    "    labels = torch.ones_like(labels) * labels_mask\n",
    "\n",
    "    if recall != 1:\n",
    "        # Evaluation model. We could slow down.\n",
    "        # labels = batch x seq x target length\n",
    "\n",
    "        logits = logits.topk(k=recall, dim = -1)[1].data.cpu().numpy()\n",
    "\n",
    "        counter = 0.0\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        for i in range(logits.shape[0]):\n",
    "            for j in range(logits.shape[1]):\n",
    "                possibles = logits[i][j]\n",
    "                current_label = labels[i][j][possibles]\n",
    "                if current_label.sum(-1) != 0:\n",
    "                    counter += 1\n",
    "        counter = torch.Tensor([counter]).cuda()\n",
    "        return counter, upper_bound_labels\n",
    "\n",
    "    logits = torch.max(logits, -1)[1].data # argmax\n",
    "    logits = logits.unsqueeze(-1)\n",
    "    scores = torch.gather(input = labels, dim = 2, index = logits)\n",
    "    scores = scores.view(-1).sum(-1)\n",
    "    return scores, upper_bound_labels\n",
    "\n",
    "def transform_to_batch_sequence(tensor):\n",
    "    if tensor is not None:\n",
    "        if len(tensor.size()) == 2:\n",
    "            return tensor\n",
    "        else:\n",
    "            assert(len(tensor.size()) == 3)\n",
    "            return tensor.contiguous().view(-1, tensor.size(-1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def transform_to_batch_sequence_dim(tensor):\n",
    "    if tensor is not None:\n",
    "        if len(tensor.size()) == 3:\n",
    "            return tensor\n",
    "        else:\n",
    "            assert(len(tensor.size()) == 4)\n",
    "            return tensor.contiguous().view(-1, tensor.size(-2), tensor.size(-1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def masked_unk_softmax(x, dim, mask_idx):\n",
    "    x1 = F.softmax(x, dim=dim)\n",
    "    x1[:, mask_idx] = 0\n",
    "    x1_sum = torch.sum(x1, dim=1, keepdim=True)\n",
    "    y = x1 / x1_sum\n",
    "    return y\n",
    "\n",
    "def compute_score_with_logits(logits, labels):\n",
    "    logits = masked_unk_softmax(logits, 1, 0)\n",
    "    logits = torch.max(logits, 1)[1].data  # argmax\n",
    "    one_hots = torch.zeros_like(labels)\n",
    "    one_hots.scatter_(1, logits.view(-1, 1), 1)\n",
    "    scores = (one_hots * labels)\n",
    "    return scores\n",
    "\n",
    "def batched_index_select(t, dim, inds):\n",
    "    dummy = inds.unsqueeze(2).expand(inds.size(0), inds.size(1), t.size(2))\n",
    "    out = t.gather(dim, dummy) # b x e x f\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
