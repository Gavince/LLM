# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-02-21 21:08+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.14.0\n"

#: ../../source/run_locally/ollama.rst:2 5dbfc8ae09a24c89964f37ebd4ca66bd
msgid "Ollama"
msgstr "Ollama"

#: ../../source/run_locally/ollama.rst:4 ba8364b5fce64dedabeb7cfbcf677675
msgid ""
"`Ollama <https://ollama.com/>`__ helps you run LLMs locally with only a "
"few commands. It is available at MacOS, Linux, and Windows. Now, Qwen1.5 "
"is officially on Ollama, and you can run it with one command:"
msgstr "`Ollama <https://ollama.com/>`__ 帮助您通过少量命令即可在本地运行LLM。它适用于MacOS、Linux和Windows操作系统。现在，Qwen1.5正式上线Ollama，您只需一条命令即可运行它："

#: ../../source/run_locally/ollama.rst:12 93533ba6809b42c19a936440d86df528
msgid "Next, we introduce more detailed usages of Ollama for running Qwen models."
msgstr "接着，我们介绍在Ollama使用Qwen模型的更多用法"

#: ../../source/run_locally/ollama.rst:16 a972d55b2d0e4ec1b9046258d7342d09
msgid "Quickstart"
msgstr "快速开始"

#: ../../source/run_locally/ollama.rst:18 494ac69706fb4581b63e8eb81b91148b
msgid ""
"Visit the official website `Ollama <https://ollama.com/>`__ and click "
"download to install Ollama on your device. You can also search models in "
"the website, where you can find the Qwen1.5 models. Except for the "
"default one, you can choose to run Qwen1.5-Chat models of different sizes"
" by:"
msgstr "访问官方网站 `Ollama <https://ollama.com/>`__ ”，点击 ``Download`` 以在您的设备上安装Ollama。您还可以在网站上搜索模型，在这里您可以找到Qwen1.5系列模型。除了默认模型之外，您可以通过以下方式选择运行不同大小的Qwen1.5-Chat模型："

#: ../../source/run_locally/ollama.rst:24 2aa16e7c8f2144958e5682603c54729c
msgid "``ollama run qwen:0.5b``"
msgstr ""

#: ../../source/run_locally/ollama.rst:25 e409605f90f64c9383f2fd7401b51b69
msgid "``ollama run qwen:1.8b``"
msgstr ""

#: ../../source/run_locally/ollama.rst:26 eb4a5cd83bd944acb4576566d6ef39ab
msgid "``ollama run qwen:4b``"
msgstr ""

#: ../../source/run_locally/ollama.rst:27 53dd5fcbf9794c6fa9351ac89c59896f
msgid "``ollama run qwen:7b``"
msgstr ""

#: ../../source/run_locally/ollama.rst:28 56ed4eb4153643ccae8a59abd9b8cf99
msgid "``ollama run qwen:14b``"
msgstr ""

#: ../../source/run_locally/ollama.rst:29 b3e49968c75545f7b291dd7ae099fb8b
msgid "``ollama run qwen:72b``"
msgstr ""

#: ../../source/run_locally/ollama.rst:32 52d43c07a68b428692fbb19ccb6e1813
msgid "Run Ollama with Your GGUF Files"
msgstr "在Ollama运行你的GGUF文件"

#: ../../source/run_locally/ollama.rst:34 948e45e8c2aa4cadb9246d2a4d94c3f3
msgid ""
"Sometimes you don't want to pull models and you just want to use Ollama "
"with your own GGUF files. Suppose you have a GGUF file of Qwen, ``qwen1_5"
"-7b-chat-q4_0.gguf``. For the first step, you need to create a file "
"called ``Modelfile``. The content of the file is shown below:"
msgstr "有时您可能不想拉取模型，而是希望直接使用自己的GGUF文件来配合Ollama。假设您有一个名为 ``qwen1_5-7b-chat-q4_0.gguf`` 的Qwen的GGUF文件。在第一步中，您需要创建一个名为 ``Modelfile`` 的文件。该文件的内容如下所示："

#: ../../source/run_locally/ollama.rst:61 3843d0ede09345ce8f694bc19399b780
msgid "Then create the ollama model by running:"
msgstr "然后通过运行下列命令来创建一个ollama模型"

#: ../../source/run_locally/ollama.rst:67 c061968787654e92ae68ef9a8d2a19ed
msgid "Once it is finished, you can run your ollama model by:"
msgstr "完成后，你即可运行你的ollama模型："

