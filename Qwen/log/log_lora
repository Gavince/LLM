nohup: ignoring input
[2024-02-26 23:12:38,751] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
CUDA extension not installed.
CUDA extension not installed.
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
trainable params: 53,673,984 || all params: 676,104,192 || trainable%: 7.938714866006925
Loading data...
Formatting inputs...Skip in lazy mode
  0%|          | 0/30 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  3%|▎         | 1/30 [00:09<04:33,  9.44s/it]                                                3%|▎         | 1/30 [00:09<04:33,  9.44s/it]  7%|▋         | 2/30 [00:18<04:14,  9.09s/it]                                                7%|▋         | 2/30 [00:18<04:14,  9.09s/it] 10%|█         | 3/30 [00:27<04:03,  9.01s/it]                                               10%|█         | 3/30 [00:27<04:03,  9.01s/it] 13%|█▎        | 4/30 [00:36<03:53,  8.97s/it]                                               13%|█▎        | 4/30 [00:36<03:53,  8.97s/it] 17%|█▋        | 5/30 [00:45<03:44,  8.97s/it]                                               17%|█▋        | 5/30 [00:45<03:44,  8.97s/it] 20%|██        | 6/30 [00:54<03:35,  8.97s/it]                                               20%|██        | 6/30 [00:54<03:35,  8.97s/it] 23%|██▎       | 7/30 [01:03<03:26,  8.99s/it]                                               23%|██▎       | 7/30 [01:03<03:26,  8.99s/it] 27%|██▋       | 8/30 [01:12<03:18,  9.00s/it]                                               27%|██▋       | 8/30 [01:12<03:18,  9.00s/it] 30%|███       | 9/30 [01:21<03:09,  9.01s/it]                                               30%|███       | 9/30 [01:21<03:09,  9.01s/it] 33%|███▎      | 10/30 [01:30<03:00,  9.02s/it]                                                33%|███▎      | 10/30 [01:30<03:00,  9.02s/it]/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /root/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4 - will assume that the vocabulary was not modified.
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 37%|███▋      | 11/30 [01:39<02:55,  9.23s/it]                                                37%|███▋      | 11/30 [01:39<02:55,  9.23s/it] 40%|████      | 12/30 [01:48<02:45,  9.18s/it]                                                40%|████      | 12/30 [01:48<02:45,  9.18s/it] 43%|████▎     | 13/30 [01:57<02:35,  9.14s/it]                                                43%|████▎     | 13/30 [01:57<02:35,  9.14s/it] 47%|████▋     | 14/30 [02:07<02:25,  9.11s/it]                                                47%|████▋     | 14/30 [02:07<02:25,  9.11s/it] 50%|█████     | 15/30 [02:16<02:16,  9.09s/it]                                                50%|█████     | 15/30 [02:16<02:16,  9.09s/it] 53%|█████▎    | 16/30 [02:25<02:07,  9.07s/it]                                                53%|█████▎    | 16/30 [02:25<02:07,  9.07s/it] 57%|█████▋    | 17/30 [02:34<01:57,  9.05s/it]                                                57%|█████▋    | 17/30 [02:34<01:57,  9.05s/it] 60%|██████    | 18/30 [02:43<01:48,  9.03s/it]                                                60%|██████    | 18/30 [02:43<01:48,  9.03s/it] 63%|██████▎   | 19/30 [02:52<01:39,  9.02s/it]                                                63%|██████▎   | 19/30 [02:52<01:39,  9.02s/it] 67%|██████▋   | 20/30 [03:01<01:30,  9.01s/it]                                                67%|██████▋   | 20/30 [03:01<01:30,  9.01s/it]/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /root/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4 - will assume that the vocabulary was not modified.
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 70%|███████   | 21/30 [03:10<01:22,  9.22s/it]                                                70%|███████   | 21/30 [03:10<01:22,  9.22s/it] 73%|███████▎  | 22/30 [03:19<01:13,  9.15s/it]                                                73%|███████▎  | 22/30 [03:19<01:13,  9.15s/it] 77%|███████▋  | 23/30 [03:28<01:03,  9.10s/it]                                                77%|███████▋  | 23/30 [03:28<01:03,  9.10s/it] 80%|████████  | 24/30 [03:37<00:54,  9.07s/it]                                                80%|████████  | 24/30 [03:37<00:54,  9.07s/it] 83%|████████▎ | 25/30 [03:46<00:45,  9.05s/it]                                                83%|████████▎ | 25/30 [03:46<00:45,  9.05s/it] 87%|████████▋ | 26/30 [03:55<00:36,  9.03s/it]                                                87%|████████▋ | 26/30 [03:55<00:36,  9.03s/it] 90%|█████████ | 27/30 [04:04<00:27,  9.02s/it]                                                90%|█████████ | 27/30 [04:04<00:27,  9.02s/it] 93%|█████████▎| 28/30 [04:13<00:18,  9.01s/it]                                                93%|█████████▎| 28/30 [04:13<00:18,  9.01s/it] 97%|█████████▋| 29/30 [04:22<00:09,  9.01s/it]                                                97%|█████████▋| 29/30 [04:22<00:09,  9.01s/it]100%|██████████| 30/30 [04:31<00:00,  9.00s/it]                                               100%|██████████| 30/30 [04:31<00:00,  9.00s/it]/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /root/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4 - will assume that the vocabulary was not modified.
  warnings.warn(
                                               100%|██████████| 30/30 [04:32<00:00,  9.00s/it]100%|██████████| 30/30 [04:32<00:00,  9.08s/it]
/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /root/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4 - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.4739, 'learning_rate': 0.0003, 'epoch': 0.06}
{'loss': 0.4227, 'learning_rate': 0.00029912069357315393, 'epoch': 0.13}
{'loss': 0.1368, 'learning_rate': 0.000296493083356513, 'epoch': 0.19}
{'loss': 0.0331, 'learning_rate': 0.00029214797567742035, 'epoch': 0.26}
{'loss': 0.0322, 'learning_rate': 0.0002861363129506435, 'epoch': 0.32}
{'loss': 0.0355, 'learning_rate': 0.00027852857642513836, 'epoch': 0.38}
{'loss': 0.018, 'learning_rate': 0.00026941395985584653, 'epoch': 0.45}
{'loss': 0.0126, 'learning_rate': 0.0002588993237884696, 'epoch': 0.51}
{'loss': 0.0116, 'learning_rate': 0.00024710794271727413, 'epoch': 0.58}
{'loss': 0.0061, 'learning_rate': 0.00023417805980435736, 'epoch': 0.64}
{'loss': 0.0046, 'learning_rate': 0.00022026126610496852, 'epoch': 0.7}
{'loss': 0.0034, 'learning_rate': 0.00020552072330098716, 'epoch': 0.77}
{'loss': 0.0043, 'learning_rate': 0.00019012925077938314, 'epoch': 0.83}
{'loss': 0.0016, 'learning_rate': 0.00017426729948291474, 'epoch': 0.9}
{'loss': 0.0013, 'learning_rate': 0.0001581208362878126, 'epoch': 0.96}
{'loss': 0.0013, 'learning_rate': 0.00014187916371218736, 'epoch': 1.02}
{'loss': 0.0022, 'learning_rate': 0.0001257327005170853, 'epoch': 1.09}
{'loss': 0.0007, 'learning_rate': 0.00010987074922061689, 'epoch': 1.15}
{'loss': 0.001, 'learning_rate': 9.447927669901282e-05, 'epoch': 1.22}
{'loss': 0.0007, 'learning_rate': 7.973873389503149e-05, 'epoch': 1.28}
{'loss': 0.0006, 'learning_rate': 6.582194019564266e-05, 'epoch': 1.34}
{'loss': 0.0005, 'learning_rate': 5.289205728272586e-05, 'epoch': 1.41}
{'loss': 0.0007, 'learning_rate': 4.1100676211530404e-05, 'epoch': 1.47}
{'loss': 0.0006, 'learning_rate': 3.058604014415343e-05, 'epoch': 1.54}
{'loss': 0.0006, 'learning_rate': 2.147142357486164e-05, 'epoch': 1.6}
{'loss': 0.0004, 'learning_rate': 1.3863687049356464e-05, 'epoch': 1.66}
{'loss': 0.0006, 'learning_rate': 7.852024322579648e-06, 'epoch': 1.73}
{'loss': 0.0007, 'learning_rate': 3.506916643487001e-06, 'epoch': 1.79}
{'loss': 0.0005, 'learning_rate': 8.793064268460604e-07, 'epoch': 1.86}
{'loss': 0.0004, 'learning_rate': 0.0, 'epoch': 1.92}
{'train_runtime': 272.4379, 'train_samples_per_second': 7.341, 'train_steps_per_second': 0.11, 'train_loss': 0.04030166444038817, 'epoch': 1.92}
